{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "- include_top, false\n",
    "- finetune with trainable layers, true\n",
    "- keep images large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Conv2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "from squeezenet import SqueezeNet, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/goodscifi/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import helper_keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14682023978636540792\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7862963405\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1467946150659410101\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Make sure tensorflow is using GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "notbook_path = '/home/jason/DeepLearning/github/goodscifi/development/notebooks/'\n",
    "home_path = '/home/jason/DeepLearning/github/goodscifi/development/'\n",
    "data_path = '/home/jason/DeepLearning/github/goodscifi/development/data/'\n",
    "model_path = '/home/jason/DeepLearning/github/goodscifi/development/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data is imbalance try using unsamples found in train_undersample/\n",
    "training_data = data_path+'dataset/books/train/'\n",
    "validation_data = data_path+'dataset/books/valid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image Preprocessing and data augmentation\n",
    "image_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "#                                horizontal_flip=True\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batches_aug = helper_keras.get_batches(training_data,\n",
    "                                            gen=image_gen, \n",
    "                                            batch_size=128,\n",
    "                                            target_size=(227,227)) \n",
    "val_batches_aug = helper_keras.get_batches(validation_data,\n",
    "                                            gen=image_gen, \n",
    "                                            batch_size=128,\n",
    "                                            target_size=(227,227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set steps per epoch for train & validation data\n",
    "steps_per_epoch = train_batches_aug.samples/train_batches_aug.batch_size\n",
    "val_steps = val_batches_aug.samples/val_batches_aug.batch_size\n",
    "\n",
    "print('steps per epoch:', steps_per_epoch)\n",
    "print('validation steps:', val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cls_weight = helper_keras.get_class_ratio(training_data); cls_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Func**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Flatten vs GlobalAveragePooling2D() in this context\n",
    "def add_new_last_layer(base_model,classes=2):\n",
    "#     x = base_model.output\n",
    "#     x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(512)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     x = Dense(classes)(x)\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = Conv2D(classes, (1, 1))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    predictions = Activation('softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_finetuning(model, num_layers_to_freeze):\n",
    "    for layer in model.layers[:num_layers_to_freeze]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[num_layers_to_freeze:]:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add to helper\n",
    "def setup_transfer_learning(base_model, model):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add to helper\n",
    "def create_callbacks(run_label):\n",
    "    outputFolder = model_path+'squeezenet/'\n",
    "    if not os.path.exists(outputFolder):\n",
    "        os.makedirs(outputFolder)\n",
    "    filepath=outputFolder+\"weights.best.hdf5\"\n",
    "    \n",
    "    # Checkpoint callback\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
    "                                 save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    # Early stopping callback\n",
    "    earlystop = EarlyStopping(monitor='val_loss', patience=40,verbose=1, mode='auto')\n",
    "    \n",
    "    # Tensorboard callback\n",
    "    tensorboard = TensorBoard(log_dir='./logs/squeeze_'+str(run_label))\n",
    "    \n",
    "    return [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = SqueezeNet(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "setup_finetuning(base_model, 47) # 54, 47, 40\n",
    "for layer in base_model.layers:\n",
    "    if layer.trainable == True:\n",
    "        print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr_to_test = [1e-06]\n",
    "cls_weight = {0:8, 1:1}\n",
    "transfer_learn = False\n",
    "r_label = 'run1_1e-06'\n",
    "\n",
    "# trainable layers [9, 8, 7]\n",
    "layers_to_freeze = [47]\n",
    "\n",
    "# Loop over learning rates\n",
    "for ltf in layers_to_freeze:\n",
    "    print('\\n\\nTesting model with ltf parameter: %f\\n'%ltf )\n",
    "    \n",
    "    # Set run label\n",
    "    run_label = r_label+'_ltf-'+str(ltf)\n",
    "    \n",
    "    # Create new model\n",
    "    base_model = SqueezeNet(weights='imagenet', include_top=False, pooling=None)\n",
    "    base_model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    model = add_new_last_layer(base_model)\n",
    "    \n",
    "    if transfer_learn:\n",
    "        setup_transfer_learning(base_model, model)\n",
    "    else:\n",
    "        setup_finetuning(model, ltf)\n",
    "    \n",
    "    # Optimize\n",
    "    # Use SGD to start - find the best appropriate lr for Adam\n",
    "    optimizer = Adam(lr=lr_to_test)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks_list = create_callbacks(run_label)\n",
    "\n",
    "    # Fit the model\n",
    "    start = time.time()\n",
    "    print(\"Model started training at\",time.ctime())\n",
    "    history =  model.fit_generator(train_batches_aug,\n",
    "                                    steps_per_epoch=steps_per_epoch,\n",
    "                                    validation_data=val_batches_aug,\n",
    "                                    validation_steps=val_steps,\n",
    "                                    epochs=500,\n",
    "                                    verbose=0,\n",
    "                                    callbacks= callbacks_list,\n",
    "                                    class_weight=cls_weight)\n",
    "    end = time.time()\n",
    "    print(\"Model took %0.2f seconds to train\"%(end - start))\n",
    "    # plot model history\n",
    "    helper_keras.plot_model_history(history)\n",
    "    # Save model+weights(last known configs)\n",
    "    model.save(model_path+'squeeze_'+str(run_label)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer = Adam(lr=0.000000001)\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '/home/jason/DeepLearning/github/goodscifi/development/models/squeeze_run6_1e-06_ltf-47.h5'\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run 7 at batch_size 128, no data augmentation\n",
    "run_label = 'run7_1e-06_ltf-47'\n",
    "callbacks_list = create_callbacks(run_label)\n",
    "\n",
    "history =  model.fit_generator(train_batches_aug,\n",
    "                                steps_per_epoch=steps_per_epoch,\n",
    "                                validation_data=val_batches_aug,\n",
    "                                validation_steps=val_steps,\n",
    "                                epochs=500,\n",
    "                                verbose=0,\n",
    "                                callbacks=callbacks_list,\n",
    "                                class_weight=cls_weight)\n",
    "\n",
    "model.save(model_path+'squeeze_'+str(run_label)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCE (goodscifi)",
   "language": "python",
   "name": "goodscifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
