{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoodSciFi - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get images of all sci-fi\n",
    "2. Get lists of all \"classic,greatest\"\n",
    "3. Use lists to \"label\" the associated posters/covers as \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "Data cleanup and overview of all data gathered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Review\n",
    "Although some data cleanup/processing was performed during the web scraping processing, such as replacing spaces with dashes, this should be double checked along with items such as Null or None fields, and characters which may cause issues down stream. One of the most important items which may cause issues downstream are duplicate entries in the lists. This will be dealt with here as well.\n",
    "\n",
    "1. Get list of dataset names (.json)\n",
    "2. Sample \"normal\" list vs list with image references\n",
    "3. Determine columns and what to drop\n",
    "4. Copy all lists into dataframes with appropriate columns\n",
    " - Remove duplicates at the same time if possible\n",
    "6. In DF, remove duplicate rows based on x columns\n",
    "7. Review null/none colum entries, drop where appropriate\n",
    "8. Once completed, save (pickle?) dataframes into processed folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, json, re, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from shutil import copyfile\n",
    "import utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jason/DeepLearning/github/goodscifi/development\n"
     ]
    }
   ],
   "source": [
    "# Get current top level directory\n",
    "% cd ..\n",
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up data folder paths\n",
    "data_dir = current_dir+'/data/'\n",
    "dataset_dir = data_dir+'dataset/'\n",
    "data_raw = current_dir+'/data/raw/'\n",
    "data_processed = current_dir+'/data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take a peek into all json files\n",
    "[x.split('/')[-1] for x in utils.glob(data_raw+'*.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have a list of all scraped files. And below, additional information regarding the naming convention and data found within each file category.\n",
    "\n",
    "**Books**\n",
    "\n",
    "Filenames with \"all_books\" contains are base of all books science fiction books written (based on the knowledge of the source). There are two such lists, we'll only be using the \"wwend_all_books.json\" file as the other references smaller images sizes. \n",
    "\n",
    "The \"top\" lists of books are represented by the name format of \"books_\" and then the source. For example, the file books_goodreads.json contains data regarding science fiction books from the website goodreads.com\n",
    "\n",
    "**Movies and TV Shows**\n",
    "\n",
    "Movie and TV Show data can be found together in the lists prefixed with \"movies_\" where appropriate. For example, the website On DVD Releases, only provides the sci-fi movies on DVD so the list, movies_ondvd.json, only contains movies. In contract, the file \"movies_imdb.json\" contains data from both movies and tv shows.\n",
    "\n",
    "\n",
    "**Source of Data**\n",
    "\n",
    "A brief explanation of the data source. Although in some cases it's pretty straight forward, others such as data from the file books_goodreads.json is abit mysterious. The website Goodreads allows users to create lists, vote and rank entries. I reviewed a number of science fiction lists and picked three of the most updated, voted on, commented and \"appropriate\" (e.g. all-time vs 90s) lists available. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base / All Images**\n",
    "\n",
    "The main source for the images (posters and covers) come from The Movie Database and World Without End. Below is a snap of what we can find in the raw output paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now lets look at our base images (posters and covers)\n",
    "# These will be later 'labelled' as good is found in the lists above\n",
    "g = glob(data_raw+'/tmdb_all_movies/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "[x.split('/')[-1] for x in shuf[100:110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob(data_raw+'/tmdb_all_tvshows/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "[x.split('/')[-1] for x in shuf[100:110]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Notice for books, we will need to stitch some data together in order to associate title with image\n",
    "# Associated title info can be found in wwend_all_books.json\n",
    "g = glob(data_raw+'/wwend_all_books/full/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "[x.split('/')[-1] for x in shuf[100:110]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review of the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of json files\n",
    "list_of_lists = [x.split('/')[-1] for x in glob(data_raw+'*.json')]; list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Review a file - 'year' can be empty\n",
    "df = pd.read_json(data_raw+list_of_lists[0]); df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ISFDB (and TMDB) contains images to be referenced\n",
    "df = pd.read_json(data_raw+list_of_lists[1]); df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here you can see 'images' is a list which contains the path to the image\n",
    "df.loc[494, 'images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load image from path\n",
    "image_path = df.loc[494, 'images'][0]['path']\n",
    "img = Image.open(data_raw+'books_isfdb/'+image_path)\n",
    "\n",
    "# Some re-sizing\n",
    "width, height = img.size\n",
    "new_width  = 200\n",
    "new_height = int(new_width * height / width)\n",
    "size = new_width, new_height\n",
    "\n",
    "# Display image\n",
    "img.resize(size, Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the Lists of Movies, TV Shows and Books**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load list of books and movies/tvshows and concat into two dataframes (books and movies)\n",
    "list_of_books = ['books_goodreads.json', 'books_isfdb.json', 'books_wwe.json']\n",
    "list_of_movies = ['movies_ign.json', 'movies_imdb.json', 'movies_ondvd.json', \n",
    "               'movies_ranker.json', 'movies_rt.json', 'movies_sff.json']\n",
    "\n",
    "lists_of_dfs = []\n",
    "for book_list in list_of_books:\n",
    "    df = pd.read_json(data_raw+book_list)\n",
    "    lists_of_dfs.append(df)\n",
    "\n",
    "df_books = pd.concat(lists_of_dfs, ignore_index=True)\n",
    "\n",
    "lists_of_dfs = []\n",
    "for movie_list in list_of_movies:\n",
    "    df = pd.read_json(data_raw+movie_list)\n",
    "    lists_of_dfs.append(df)\n",
    "    \n",
    "df_movies = pd.concat(lists_of_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_movies.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start doing some clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some fields are empty spaces, None and NaN - Drop cols and na\n",
    "# 'year' only really needed for reboots with same name\n",
    "cols_to_drop = ['image_urls', 'images', 'year']\n",
    "books = df_books.drop(cols_to_drop, axis=1)\n",
    "movies = df_movies.drop(cols_to_drop, axis=1)\n",
    "\n",
    "books = books.replace(['None','','NaN','\\s+' ,None], np.nan, regex=True).dropna(how='all')\n",
    "movies = movies.replace(['None','','NaN','\\s+' ,None], np.nan, regex=True).dropna(how='all')\n",
    "\n",
    "books.loc[:,'title'] = books.title.str.lower()\n",
    "movies.loc[:,'title'] = movies.title.str.lower()\n",
    "\n",
    "# Remove duplicate entries - title and year\n",
    "print('Total number of duplicate book titles: {}'.format(sum(books.title.duplicated())))\n",
    "print('Total number of duplicate movie+tv titles: {}'.format(sum(movies.title.duplicated())))\n",
    "\n",
    "books.drop_duplicates(inplace=True)\n",
    "movies.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies.reset_index(drop=True, inplace=True)\n",
    "books.reset_index(drop=True, inplace=True)\n",
    "print('Total number of \"good\" books: {}'.format(books.shape[0]))\n",
    "print('Total number of \"good\" movies+tv shows: {}'.format(movies.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(books, data_processed+'books.pickle')\n",
    "dump(movies, data_processed+'movies.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use ISFDB, IMDB and WWEND Title Info to Rename Image Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lists with image references - used to update image file names\n",
    "list_of_lists = ['books_isfdb.json', 'wwend_all_books.json', 'movies_imdb.json']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for book_list in list_of_lists:\n",
    "    df = pd.read_json(data_raw+book_list)\n",
    "    # TODO: disinfect title before using as filename\n",
    "    for i in range(df.shape[0]):\n",
    "        folder_name = book_list.split('.')[0]\n",
    "        src_path = data_raw+folder_name+'/'+df.loc[i,'images'][0]['path']\n",
    "        new_filename = df.loc[i,'title'].replace('/','-').lower() +'.jpg'\n",
    "        \n",
    "        copyfile(src_path, data_processed+folder_name+'/'+new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy files from tmdb tv and movies into processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all renamed images into 2 (movies and books) folder. Duplicate named files are kept (mostly different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BOOKS - move all book covers into same folder keeping dups (some have diff covers, same book)\n",
    "list_of_books = ['books_isfdb', 'wwend_all_books']\n",
    "for book_list in list_of_books:\n",
    "    g = glob(data_processed+'/'+book_list+'/*.jpg')\n",
    "    for i in range(len(g)):\n",
    "        file_name = g[i].split('/')[-1].split('.jpg')[0]+'_'+str(i)+'.jpg'\n",
    "        copyfile(g[i], dataset_dir+'books/train/'+file_name)\n",
    "    \n",
    "\n",
    "# MOVIES/TV SHOWS - move posters into same folder keeping dups (some have diff posters, same show)\n",
    "list_of_movies_shows = ['tmdb_all_tvshows', 'tmdb_all_movies', 'movies_imdb']\n",
    "for movie_list in list_of_movies_shows:\n",
    "    g = glob(data_processed+'/'+movie_list+'/*.jpg')\n",
    "    for i in range(len(g)):\n",
    "        file_name = g[i].split('/')[-1].split('.jpg')[0]+'_'+str(i)+'.jpg'\n",
    "        copyfile(g[i], dataset_dir+'movies/train/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean all filenames\n",
    "import re\n",
    "def clean_title(title):\n",
    "    if title is None: return None\n",
    "\n",
    "    title = re.sub('[/:+=,]','-',title)\n",
    "    title = title.lstrip('@,%,+,-,#,!')\n",
    "    title = title.strip()\n",
    "    title = title.replace(' ', '-')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = glob(dataset_dir+'books/train/*.jpg')\n",
    "for i in range(len(g)):\n",
    "    file_name = clean_title(g[i].split('/')[-1]).lower()\n",
    "    os.rename(g[i], dataset_dir+'books/train/'+file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload:** Movie and Book dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b_df = load(data_processed+'books.pickle')\n",
    "m_df = load(data_processed+'movies.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(b_df.shape)\n",
    "print(m_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_df['title'] = m_df['title'].apply(clean_title)\n",
    "b_df['title'] = b_df['title'].apply(clean_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Glob file paths, convert to dataframe and add title (filename) column\n",
    "# Then merge dataframes with \"good\" list\n",
    "g = glob(dataset_dir+'movies/train/*.jpg')\n",
    "\n",
    "# Convert list to dataframe\n",
    "file_paths = np.array(g)\n",
    "col_names = ['path']\n",
    "\n",
    "all_movies_df = pd.DataFrame(file_paths, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To match both df with 'title' need to clean up title_{year}_{tmdb_id}_index.jpg\n",
    "all_movies_df['title'] = all_movies_df.loc[:,'path'].apply(lambda x: re.split('_', re.split('/', x)[-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_movies_df.loc[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_movies_df.loc[all_movies_df.loc[:,'title'] == '2001--a-space-odyssey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge m_df with all_movies_df keeping \"good\" list and adding path\n",
    "goodscifi_movies_df = pd.merge(m_df, all_movies_df, on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodscifi_movies_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodscifi_movies_df.iloc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just to make sure these are different files\n",
    "goodscifi_movies_df.loc[goodscifi_movies_df['title'] == 'the-thing', 'path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Repeat but for books\n",
    "g = glob(dataset_dir+'books/train/*.jpg')\n",
    "file_paths = np.array(g)\n",
    "col_names = ['path']\n",
    "\n",
    "all_books_df = pd.DataFrame(file_paths, columns=col_names)\n",
    "\n",
    "# To match both df with 'title' need to clean up title_{year}_{tmdb_id}_index.jpg\n",
    "all_books_df['title'] = all_books_df.loc[:,'path'].apply(lambda x: re.split('_', re.split('/', x)[-1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_books_df.iloc[995:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge m_df with all_movies_df keeping \"good\" list and adding path\n",
    "goodscifi_books_df = pd.merge(b_df, all_books_df, on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodscifi_books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dump(goodscifi_movies_df, data_processed+'goodscifi_movies.pickle')\n",
    "dump(goodscifi_books_df, data_processed+'goodscifi_books.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reload:** Goodscifi Movie and Book dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "goodscifi_books_df = load(data_processed+'goodscifi_books.pickle')\n",
    "goodscifi_movies_df = load(data_processed+'goodscifi_movies.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1114, 2)\n",
      "(1084, 2)\n"
     ]
    }
   ],
   "source": [
    "print(goodscifi_books_df.shape)\n",
    "print(goodscifi_movies_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Books\n",
    "for i in range(goodscifi_books_df.shape[0]):\n",
    "    file_to_be_moved = goodscifi_books_df.iloc[i]['path']\n",
    "    file_name = file_to_be_moved.split('/')[-1]\n",
    "    os.rename(file_to_be_moved, dataset_dir+'books/train/good/'+file_name)\n",
    "\n",
    "g = glob(dataset_dir+'/books/train/*.jpg')\n",
    "for i in range(len(g)):\n",
    "    file_name = g[i].split('/')[-1]\n",
    "    os.rename(g[i], dataset_dir+'books/train/not_good/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Movies\n",
    "for i in range(goodscifi_movies_df.shape[0]):\n",
    "    file_to_be_moved = goodscifi_movies_df.iloc[i]['path']\n",
    "    file_name = file_to_be_moved.split('/')[-1]\n",
    "    os.rename(file_to_be_moved, dataset_dir+'movies/train/good/'+file_name)\n",
    "\n",
    "g = glob(dataset_dir+'/movies/train/*.jpg')\n",
    "for i in range(len(g)):\n",
    "    file_name = g[i].split('/')[-1]\n",
    "    os.rename(g[i], dataset_dir+'movies/train/not_good/'+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b69c9f8c5b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# movie_data_good = glob(dataset_dir+'/movies/train/good/*.jpg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# movie_data_not_good = glob(dataset_dir+'/movies/train/not_good/*.jpg')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbook_data_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/books/train/good/*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbook_data_not_good\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/books/train/not_good/*.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# Shuffle good, not_good files and split into validation and test sets\n",
    "# movie_data_good = glob(dataset_dir+'/movies/train/good/*.jpg')\n",
    "# movie_data_not_good = glob(dataset_dir+'/movies/train/not_good/*.jpg')\n",
    "book_data_good = glob(dataset_dir+'/books/train/good/*.jpg')\n",
    "book_data_not_good = glob(dataset_dir+'/books/train/not_good/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1084, 6161) (1114, 7054)\n"
     ]
    }
   ],
   "source": [
    "m = (len(movie_data_good), len(movie_data_not_good))\n",
    "b = (len(book_data_good), len(book_data_not_good))\n",
    "\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# glob all training data\n",
    "# randomly shuffle files (set seed)\n",
    "# set ratio (e.g. 60% of total training and 1/6th of good data)\n",
    "# loop over and move data to valid and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2898.0 483.0 2415.0\n",
      "241 1207\n"
     ]
    }
   ],
   "source": [
    "# Calculate numbers to move to between valid and test (60:20:20)\n",
    "m_total_take = (m[0]+m[1]) * 0.40 \n",
    "m_total_good = m_total_take * 1/6\n",
    "m_total_not_good = m_total_take - m_total_good\n",
    "\n",
    "# Split up totals between valid and test (0.5)\n",
    "m_good = math.floor(m_total_good / 2)\n",
    "m_not_good = math.floor(m_total_not_good / 2)\n",
    "\n",
    "print(m_total_take, m_total_good, m_total_not_good)\n",
    "print(m_good, m_not_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Movies: Good Data\n",
    "\n",
    "# Valid\n",
    "g = glob(dataset_dir+'movies/train/good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(m_good):\n",
    "    os.rename(shuf[i], dataset_dir+'movies/valid/good/'+shuf[i].split('/')[-1])\n",
    "\n",
    "# Test\n",
    "g = glob(dataset_dir+'movies/train/good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(m_good):\n",
    "    os.rename(shuf[i], dataset_dir+'movies/test/good/'+shuf[i].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Movies: Not Good Data\n",
    "\n",
    "# Valid\n",
    "g = glob(dataset_dir+'movies/train/not_good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(m_not_good):\n",
    "    os.rename(shuf[i], dataset_dir+'movies/valid/not_good/'+shuf[i].split('/')[-1])\n",
    "\n",
    "# Test\n",
    "g = glob(dataset_dir+'movies/train/not_good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(m_not_good):\n",
    "    os.rename(shuf[i], dataset_dir+'movies/test/not_good/'+shuf[i].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3267.2000000000003 544.5333333333334 2722.666666666667\n",
      "272 1361\n"
     ]
    }
   ],
   "source": [
    "# Calculate numbers to move to between valid and test (60:20:20)\n",
    "b_total_take = (b[0]+b[1]) * 0.40 \n",
    "b_total_good = b_total_take * 1/6\n",
    "b_total_not_good = b_total_take - b_total_good\n",
    "\n",
    "# Split up totals between valid and test (0.5)\n",
    "b_good = math.floor(b_total_good / 2)\n",
    "b_not_good = math.floor(b_total_not_good / 2)\n",
    "\n",
    "print(b_total_take, b_total_good, b_total_not_good)\n",
    "print(b_good, b_not_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Books: Good Data\n",
    "\n",
    "# Valid\n",
    "g = glob(dataset_dir+'books/train/good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(b_good):\n",
    "    os.rename(shuf[i], dataset_dir+'books/valid/good/'+shuf[i].split('/')[-1])\n",
    "\n",
    "# Test\n",
    "g = glob(dataset_dir+'books/train/good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(b_good):\n",
    "    os.rename(shuf[i], dataset_dir+'books/test/good/'+shuf[i].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Books: Not Good Data\n",
    "\n",
    "# Valid\n",
    "g = glob(dataset_dir+'books/train/not_good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(b_not_good):\n",
    "    os.rename(shuf[i], dataset_dir+'books/valid/not_good/'+shuf[i].split('/')[-1])\n",
    "\n",
    "# Test\n",
    "g = glob(dataset_dir+'books/train/not_good/*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(b_not_good):\n",
    "    os.rename(shuf[i], dataset_dir+'books/test/not_good/'+shuf[i].split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCE (goodscifi)",
   "language": "python",
   "name": "goodscifi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
